# GPU 加速 Beamsearch + 打散规则 - 最小可行性分析

## 1. 问题定义

### 当前状态
- **场景**：推荐系统混排层 beamsearch
- **候选集规模**：2000 items
- **输出规模**：100-200 items
- **打散规则**：位置相关的依赖规则
- **当前延时**：CPU 版本 20-30ms

### 核心需求
用 GPU 加速这个流程，性能目标 < 5ms（5-10 倍提升）

---

## 2. 打散规则分类

基于你的描述，有三类规则需要支持：

### 类型 A：坑位过滤/强插
```
给定位置 + 上文条件 → 对候选进行 filter
示例：首坑不出双列、位置 5 必须是视频号
```

### 类型 B：窗口 M 出 N
```
在大小为 M 的滑动窗口内，某维度最多/最少出现 N 次
示例：5 个连续 item 中，同一类目最多 2 个
维度：category_id, bizuin, itemshowtype, tag 等
```

### 类型 C：定坑折损
```
计算窗口内折损（非特定类型的占比）
示例：前 20 个位置，加热内容不超过 30%
```

---

## 3. GPU 加速的关键问题

### 问题 A：能否并行化所有位置？
**答案**：不能。规则存在依赖性。
```
位置 0：可以选任何候选
位置 1：依赖位置 0 的结果（know category_id, bizuin 等）
位置 2：依赖位置 0-1 的结果
...

因此必须串行推进位置。
```

### 问题 B：串行推进下如何利用 GPU？
**答案**：每个位置内并行。
```
位置 0：
  ├─ GPU 并行：检查 2000 个候选
  ├─ GPU 并行：计算规则有效性
  └─ CPU：选择最高分

位置 1：
  ├─ GPU 并行：检查 1999 个候选（除去已选的）
  ├─ GPU 并行：计算规则有效性
  └─ CPU：选择最高分
  
...
```

### 问题 C：哪些计算特别适合 GPU？
**候选**：
1. **广播操作** - 5 个已选 vs 2000 个候选的两两比较
   - CPU：O(5 × 2000) = 10K 操作，~10ms
   - GPU：向量化，~0.5ms（20x 提升）

2. **向量聚合** - 统计匹配结果
   - CPU：~5ms
   - GPU：~0.5ms（10x 提升）

3. **条件判断** - 2000 个候选的规则检查
   - CPU：逐个检查，~15ms
   - GPU：并行，~2ms（7.5x 提升）

---

## 4. 核心设计决策

### 决策 1：何时同步 CPU↔GPU？

**选项 A**：每个候选同步一次
```
性能：差，同步开销 >> 计算时间
```

**选项 B**：每个位置同步一次（推荐）
```
同步内容：2KB 的 bool 掩码（有效性）
同步时间：~0.1ms
总计算时间：3-5ms
同步占比：2-3%
```

**选项 C**：100 个位置同步一次
```
不可行，违反串行推进的要求
```

### 决策 2：用什么框架？

| 框架 | 性能 | 易用性 | 学习成本 | 集成度 |
|------|------|--------|---------|-------|
| CUDA C++ | ⭐⭐⭐⭐⭐ | ⭐ | 高 | 中 |
| CuPy | ⭐⭐⭐⭐ | ⭐⭐⭐ | 低 | 低 |
| TensorFlow | ⭐⭐⭐ | ⭐⭐⭐ | 中 | 高 |
| PyTorch | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 中 | 中 |

**关键问题**：
- 现有推荐系统用什么框架？
- 团队的 GPU 编程经验？
- 需要多高的集成度？
- 极致性能 vs 快速迭代？

### 决策 3：规则检查在哪里进行？

**选项 A**：CPU 检查规则，GPU 只做评分
```
规则检查：15ms（CPU）
评分计算：1ms（GPU）
总计：~16ms
收益：不显著
```

**选项 B**：GPU 检查规则，GPU 做评分
```
规则检查：2ms（GPU）
评分计算：1ms（GPU）
总计：~3ms
收益：显著 ✓
```

**选项 B 可行性**：
- 需要将规则参数转为 GPU 张量
- 需要高效的条件判断内核
- 需要处理分支分化问题

---

## 5. 初步方案框架

### 整体流程

```
输入：2000 个候选 + 规则

For 位置 from 0 to 99:
  
  1. 准备数据（CPU → GPU）
     - 已选序列维度值
     - 所有规则参数
     
  2. 规则检查（GPU）
     ├─ 坑位过滤规则
     ├─ 窗口规则（广播比较）
     └─ 折损规则
     → 输出：bool[2000] 有效掩码
     
  3. 评分计算（GPU）
     → 输出：float[2000] 评分
     
  4. 选择（CPU）
     - 应用掩码
     - argmax 选择最高分
     
  5. 更新状态（CPU）
     - result.append(selected_item)
     - 位置推进
```

### 关键优化点

1. **数据转移**：最小化次数和数据量
   - 候选特征：提前转到 GPU
   - 已选序列：累积在 GPU
   - 只在位置推进时转回 CPU

2. **计算融合**：多个规则合并成一个 GPU 操作
   - 避免多次内核启动开销
   - 利用 GPU 自动图优化

3. **并行度**：充分利用 GPU 线程
   - 2000 个候选需要 2000 个线程
   - 现代 GPU 有 2000-10000 个线程
   - 并行度充分

---

## 6. 待决策事项

### 必须确认
- [ ] **打散规则的完整列表** - 需要你的 Google Sheet
- [ ] **候选 item 的完整属性** - 除了列表中的，还有其他字段吗？
- [ ] **推荐系统架构** - 现有系统用什么框架？（TF/PyTorch/其他）
- [ ] **性能目标** - 3-5ms 够吗？还是需要 < 1ms？

### 需要讨论
- [ ] **框架选择** - 基于上面的信息做决策
- [ ] **集成方式** - 作为独立模块还是深度集成？
- [ ] **规则管理** - 配置文件方式还是代码方式？
- [ ] **回退方案** - 如果 GPU 故障的降级策略？

---

## 7. 估算数据

### 预期性能（基于理论分析）

```
CPU 版本：
  - 规则检查：15-20ms
  - 评分计算：5-10ms
  - 同步开销：1-2ms
  总计：20-30ms ❌

GPU 版本（初步估计）：
  - 规则检查（GPU）：2-3ms
  - 评分计算（GPU）：0.5-1ms
  - 同步开销：1-2ms
  总计：4-6ms ✓

性能提升：4-7 倍
```

### 开发周期估计

```
信息收集 & 架构设计：1 周
原型实现 & 性能基准：1-2 周
优化 & 集成测试：1-2 周
生产部署 & A/B：1-2 周

总计：4-7 周（1-2 人）
```

---

## 8. 下一步

### 立即需要你提供

1. **打散规则详细清单**
   - 规则类型（坑位/窗口/折损）
   - 规则参数（window_size, max_count, dimensions）
   - 优先级和互联关系

2. **候选 item 的属性**
   ```
   当前了解的：
   - score, itemshowtype, category_id, bizuin, is_heat
   
   需要确认：
   - 还有其他维度吗？
   - 是否有动态计算的属性？
   - 内存布局如何优化？
   ```

3. **系统架构信息**
   ```
   - 推荐系统用 TensorFlow/PyTorch/其他？
   - 是否已有 GPU 推理流程？
   - 显存约束是什么？
   ```

### 讨论议题

1. **框架选择** - 基于上面的信息，选择 CUDA/CuPy/TensorFlow/PyTorch
2. **优化目标** - 极致性能 vs 快速迭代的权衡
3. **风险管理** - 如何处理 GPU 故障和降级？
4. **监控方案** - 如何监测 GPU 版本和 CPU 版本的差异？

---

## 问题清单

**待回答**：
- [ ] 你的打散规则具体是什么？
- [ ] 候选 item 除了列出的，还有其他属性吗？
- [ ] 现有系统用什么框架？
- [ ] 3-5ms 的目标是硬性的吗？
- [ ] 团队是否有 GPU 编程经验？

**待确认**：
- [ ] 是否需要支持多 GPU？
- [ ] 规则是静态的还是动态的？
- [ ] 是否有实时更新规则的需求？
- [ ] 是否需要版本控制和回滚？

---

**准备好了吗？提供上面的信息，我们一起设计最优方案。** 🚀
